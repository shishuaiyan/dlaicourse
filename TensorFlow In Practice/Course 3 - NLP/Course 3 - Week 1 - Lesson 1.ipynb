{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Course 3 - Week 1 - Lesson 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaCMcjMQifQc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "93e434ef-9f07-4dd1-a761-2dbb5ce4d1bd"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)  # 编码库中前num_words个单词\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUV6QlmT8mia",
        "colab_type": "text"
      },
      "source": [
        "Notes：\n",
        "- 去掉标点符号和空格\n",
        "- 对大小写不敏感，全部字母转为小写\n",
        "- word_index按照数据库中单词出现的频率从高到低排列（多次出现的love,my,i,dog在前）\n",
        "\n",
        "Problem：  \n",
        "- num_words参数无效？   可能是数据集太少？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiPN5Wjb7Jck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8cf866bb-6093-42f5-c9c7-0686d3815872"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!',\n",
        "    'this is a test'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 4)  # 编码库中前num_words个单词\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6, 'this': 7, 'is': 8, 'a': 9, 'test': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}